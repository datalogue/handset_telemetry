{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will:\n",
    "\n",
    "* [Import Datalogue,](#Import) [connect and log in to Datalogue instance](#Connect)\n",
    "* [Connect to a datastore with training data](#Connect_Store)\n",
    "* [Create a relevant ontology](#Ontology)\n",
    "* [Attach training data to the ontology](#Training_Data)\n",
    "* [Train a model](#Train)\n",
    "* [Attach data to be used in pipelining](#Data2)\n",
    "* [Use that model in pipelines](#Pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the SDK\n",
    "<a id='Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datalogue.models.permission'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-352fe6f07a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatalogue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDtlCredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatalogue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataRef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatalogue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermission\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Import Datalogue Bag of Tricks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datalogue.models.permission'"
     ]
    }
   ],
   "source": [
    "# Import Datalogue libraries \n",
    "from datalogue import *\n",
    "from datalogue.version import __version__\n",
    "from datalogue.models.ontology import *\n",
    "from datalogue.models.datastore_collection import *\n",
    "from datalogue.models.datastore import *\n",
    "from datalogue.models.datastore import GCSDatastoreDef \n",
    "from datalogue.models.credentials import *\n",
    "from datalogue.models.stream import *\n",
    "from datalogue.models.transformations import *\n",
    "from datalogue.models.transformations.structure import *\n",
    "from datalogue.dtl import Dtl, DtlCredentials\n",
    "from datalogue.models.training import DataRef\n",
    "from datalogue.models.permission import Permission\n",
    "\n",
    "# Import Datalogue Bag of Tricks\n",
    "from DTLBagOTricks import DTL as DTLHelper\n",
    "\n",
    "\n",
    "# Import other useful libraries\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "import pandas\n",
    "from IPython.display import Image\n",
    "\n",
    "# Checks the version of the SDK is correct\n",
    "# The expected version is 0.28.3\n",
    "# If the SDK is not installed, run `! pip install datalogue` and restart the Jupyter Notebook kernel\n",
    "# If the wrong versions is installed, run `! pip install datalogue --upgrade` and restart the Jupyter Notebook kernel\n",
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Connect'></a>\n",
    "# Connect to Datalogue Install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these before launching jupyter:\n",
    "\n",
    "``` bash\n",
    "export DTL_EMAIL=\"your email\"\n",
    "export DTL_PASSWORD=\"your password\"\n",
    "```\n",
    "\n",
    "This allows you to store your username and password outside of your notebook.\n",
    "\n",
    "You may need to VPN into the machine that DTL is running on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datalogue v0.29.7-CAP1\n",
      "Logged in 'https://charter-training.dtl.systems/api' with 'johanan@datalogue.io' account)\n"
     ]
    }
   ],
   "source": [
    "# Set host, username and password variables\n",
    "\n",
    "datalogue_host = \"https://charter-training.dtl.systems\"  # for connecting to Charter training (note)\n",
    "email = \"johanan@datalogue.io\"\n",
    "password = \"1514fifteenBANG!\"\n",
    "\n",
    "# Log in to Datalogue\n",
    "BOT = DTLHelper(datalogue_host, email, password)\n",
    "dtl = BOT.dtl\n",
    "\n",
    "# Expected output Datalogue v0.28.3\n",
    "# \"Logged in '[host location]' with '[username]' account)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error getting pipelines (multiple) :: 'DtlError' object is not iterable\n",
      "\n",
      "Datalogue Server Summary :: Stores: 37, Collections: 5, Streams: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe current server state (data stores and collections, active data streams)\n",
    "BOT.server_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: clear old datastores and collections\n",
    "\n",
    "<span style=\"color:red\"> Warning! this is a non-reversible step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning! this will clean all your datastores and data collections and credentials\n",
    "\n",
    "# # Clear Datastores and Datastore Collections\n",
    "# for store in dtl.datastore.list():\n",
    "#     dtl.datastore.delete(store.id)\n",
    "# for store in dtl.datastore_collection.list():\n",
    "#     dtl.datastore_collection.delete(store.id)\n",
    "\n",
    "# # Clear credentials\n",
    "# for credential in dtl.credentials.admin.list():\n",
    "#     dtl.credentials.admin.delete(credential.id)\n",
    "    \n",
    "# # Clear data pipelines\n",
    "# for StreamCollection in dtl.stream_collection.list():\n",
    "#     dtl.stream_collection.delete(StreamCollection.id)\n",
    "\n",
    "## Clear ontologies\n",
    "# for Ontology in dtl.ontology.list():\n",
    "#     dtl.ontology.delete(ontology.id)\n",
    "\n",
    "BOT.server_summary()\n",
    "\n",
    "# After running the above, the Stores and Collections variables should both be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Connect_Store'></a>\n",
    "# Attach relevant datastore(s) for use as training data\n",
    "Here, we will:\n",
    "\n",
    "1. add credentials to allow access to the data\n",
    "2. connect to data stores\n",
    "3. Collect the training data into a data collection\n",
    "\n",
    "Note, this assumes that you are adding a CSV file from GCS as a data store, if you have other data store types you would like to add, see the [SDK Documentation](https://datalogue.readme.io/reference#datastore-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add credentials to Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these before launching jupyter:\n",
    "\n",
    "``` bash\n",
    "export GCS_EMAIL=\"your email\"\n",
    "export GCS_PASSWORD=\"your key\"\n",
    "```\n",
    "\n",
    "This allows you to store your username and password outside of your notebook.\n",
    "\n",
    "Note: these aren't required if your datastore is a HTTP or Server Local File Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_CredentialsClient' object has no attribute 'share'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e01d94d8374b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_gcs_credentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_CredentialsClient' object has no attribute 'share'"
     ]
    }
   ],
   "source": [
    "# Create local credential definition\n",
    "\n",
    "gcs_credentials_def = GCS(\n",
    "    client_email=environ.get(\"GCS_EMAIL\"),\n",
    "    private_key=environ.get(\"GCS_PASSWORD\").replace(\"\\\\n\", \"\\n\"),\n",
    ")\n",
    "\n",
    "# Create credentials in datalogue platform\n",
    "\n",
    "my_gcs_credentials = dtl.credentials.admin.create(\n",
    "    credentials_definition=gcs_credentials_def, name=\"Telemetry GCS Credentials\"\n",
    ")\n",
    "\n",
    "# Share credentials with all users of Datalogue deployment\n",
    "# This is a necessary step even if you are the only user of the credentials\n",
    "\n",
    "groups = dtl.group.get_list()\n",
    "for group in groups:\n",
    "    dtl.credentials.admin.share(my_gcs_credentials.id, group.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return table of credentials to verify above\n",
    "BOT.get_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach a datastore for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a datastore\n",
    "\n",
    "gcsdef_1 = GCSDatastoreDef(\n",
    "    bucket = \"dtl-handset-telemetry\",\n",
    "    file_name = \"5K_us_telemetry.csv\",\n",
    "    file_format = FileFormat.Csv,\n",
    ")\n",
    "\n",
    "US_handset_data = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name = \"US Telemetry 5k\", \n",
    "        definition = gcsdef_1, \n",
    "        credential_id = my_gcs_credentials.id\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting training data into Datastore Collection\n",
    "\n",
    "This collects the training data into a datastore collection for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datastore Collection objects to send to Datalogue, and send\n",
    "\n",
    "dtl.datastore_collection.create(\n",
    "    DatastoreCollection(\n",
    "        name=\"Handset telemetry training data\",\n",
    "        description=\"Handset training data for use in model creation\",\n",
    "        storeIds=[US_handset_data.id],\n",
    "    )\n",
    ")\n",
    "\n",
    "BOT.server_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview datastore by loading 5 rows into a dataframe\n",
    "\n",
    "Preview_US_Handset_table = dtl.datastore.load_arrow_table(\n",
    "  US_handset_data.id, limit = 5\n",
    ").to_pandas()\n",
    "\n",
    "Preview_US_Handset_table.head()\n",
    "\n",
    "# # print column names \n",
    "# for col in Preview_US_Handset_table.columns: \n",
    "#     print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding bulk web data for other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each HTTP source to be added\n",
    "\n",
    "data_HTTP_CSV = [\n",
    "    {\n",
    "        \"store_name\": \"Canadian Names\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500canadians.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"British Names\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500brits.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"Australian Names\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500australians.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"US Names\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500americans.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"Users\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/pii/users.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"Countries\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/countries.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"store_name\": \"Some Cities\",\n",
    "        \"URL\": \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/cities.csv\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# # This takes the above list of dictionaries and turns them into data stores\n",
    "\n",
    "for data_store in data_HTTP_CSV:\n",
    "    data_store[\"datastore_object\"] = dtl.datastore.create(\n",
    "        Datastore(\n",
    "            data_store[\"store_name\"],\n",
    "            HttpDatastoreDef(data_store[\"URL\"], FileFormat.Csv),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# BOT.server_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error getting pipelines (multiple) :: 'DtlError' object is not iterable\n",
      "\n",
      "Datalogue Server Summary :: Stores: 44, Collections: 6, Streams: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects to send to Datalogue, and send\n",
    "\n",
    "dtl.datastore_collection.create(\n",
    "    DatastoreCollection(\n",
    "        name=\"Supplemental training data (mainly customer data)\",\n",
    "        description=\"Additional data to enrich source\",\n",
    "        storeIds=[Datastore[\"datastore_object\"].id for Datastore in data_HTTP_CSV],\n",
    "    )\n",
    ")\n",
    "\n",
    "BOT.server_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCollection Name:  Telco Data\n",
      "\u001b[0m             ID:  81193bf9-db3a-496d-95e2-5efc953f3ca3\n",
      "       Contains: \n",
      "                  ➜ Churn\n",
      "                    ↪ID:  09a021f6-ff0e-4389-b6d3-4f6d7e76d721\n",
      "                  ➜ Cisco Logs\n",
      "                    ↪ID:  c754aa19-33b8-40e4-a876-74663b5657f9\n",
      "\n",
      "\u001b[1mCollection Name:  Retail Datastores\n",
      "\u001b[0m             ID:  fb71019d-250e-4b0e-a6e2-79a7dd61d666\n",
      "       Contains: \n",
      "                  ➜ Sales\n",
      "                    ↪ID:  7702de91-56b9-4fbf-ad9c-a2a1e6fe0b2e\n",
      "                  ➜ Marketing Pipe\n",
      "                    ↪ID:  e1871250-d095-442c-b2dc-8281e4c92e0f\n",
      "                  ➜ Macro Retail\n",
      "                    ↪ID:  f970d289-1202-449d-82cf-7f888b25432d\n",
      "                  ➜ Industry Descriptions\n",
      "                    ↪ID:  fb1b82d1-ba3f-4940-a3cd-71b83fb31f68\n",
      "\n",
      "\u001b[1mCollection Name:  Supplemental training data (mainly customer data)\n",
      "\u001b[0m             ID:  367b6fae-06b0-435e-b16d-b24322593a5e\n",
      "       Contains: \n",
      "                  ➜ Canadian Names\n",
      "                    ↪ID:  18914524-70e0-4119-a15b-539fae28f897\n",
      "                  ➜ Australian Names\n",
      "                    ↪ID:  40e49aad-9e72-40ba-ada3-569a3f54909b\n",
      "                  ➜ US Names\n",
      "                    ↪ID:  4ec72e4d-2052-4869-96ac-e43e21e93ddb\n",
      "                  ➜ Countries\n",
      "                    ↪ID:  5428cdff-44df-41cd-88fb-b974936664d3\n",
      "                  ➜ Some Cities\n",
      "                    ↪ID:  b636b3dd-9fae-40cc-9e87-c36206ed05c7\n",
      "                  ➜ British Names\n",
      "                    ↪ID:  b9fcd372-be19-42bb-b195-b76476dceb02\n",
      "                  ➜ Users\n",
      "                    ↪ID:  caefc78f-a3de-478d-b663-b0ba0069adb1\n",
      "\n",
      "\u001b[1mCollection Name:  New datastore for minoj\n",
      "\u001b[0m             ID:  78245086-285e-4736-aeb9-ba42e8686fe0\n",
      "       Contains: \n",
      "                  ➜ Addresses in Grand Prairie\n",
      "                    ↪ID:  0901b712-bf7d-4c0f-a1ef-6e58482e9c8f\n",
      "                  ➜ Names and locations\n",
      "                    ↪ID:  1b428ba8-b9e9-4da7-8d38-12cb9036b04f\n",
      "                  ➜ Demographics by ZIP\n",
      "                    ↪ID:  21d52f55-4e01-4158-90e2-75324a5213b2\n",
      "                  ➜ British Names\n",
      "                    ↪ID:  2e585a5b-f975-4467-9520-0120dbb2c3d8\n",
      "                  ➜ VINs and SSIDs\n",
      "                    ↪ID:  527aa194-dc79-467e-b365-9e7761123f32\n",
      "                  ➜ Addresses in Sunshine Coast\n",
      "                    ↪ID:  597e40cc-d356-46db-9090-2bf827868c33\n",
      "                  ➜ HR data with PII\n",
      "                    ↪ID:  7f4ccb94-84d2-4cb3-8230-baed3eb0304f\n",
      "                  ➜ Canadian Names\n",
      "                    ↪ID:  a54f5051-9eec-4c24-8d19-7513cd6b9c15\n",
      "                  ➜ Users\n",
      "                    ↪ID:  a608ba25-bbab-43f7-b36d-5a913313b732\n",
      "                  ➜ Company names with location and industry\n",
      "                    ↪ID:  aabcf268-77a8-48c5-89cd-38aef48f6e47\n",
      "                  ➜ Australian Names\n",
      "                    ↪ID:  bef51451-00df-4011-a98d-272576be7c32\n",
      "                  ➜ Unstructured data with PII\n",
      "                    ↪ID:  d5d3f6e3-bf91-4f59-a31d-59eb0ae8061b\n",
      "                  ➜ More names and locations\n",
      "                    ↪ID:  db650fa6-151f-4010-9e4e-a7a095c67148\n",
      "                  ➜ Customer data with PII\n",
      "                    ↪ID:  def73334-2203-4c7e-8022-4996bfcf712d\n",
      "                  ➜ Countries\n",
      "                    ↪ID:  f270330e-99e7-40bf-bc4f-e93d651e2cf6\n",
      "                  ➜ Some Cities\n",
      "                    ↪ID:  f89fda7f-60f9-4665-9154-4b6ab8eddf5d\n",
      "\n",
      "\u001b[1mCollection Name:  Personally Identifiable Information\n",
      "\u001b[0m             ID:  e203697e-777a-4477-a610-d8d9fefe3a96\n",
      "       Contains: \n",
      "                  ➜ VINs and SSIDs\n",
      "                    ↪ID:  07cbc25f-44cf-4046-bea1-9e80e5aa3a79\n",
      "                  ➜ Names and locations\n",
      "                    ↪ID:  1646b083-ead8-4976-923a-417c0f22f1a4\n",
      "                  ➜ 500 brits\n",
      "                    ↪ID:  5f4b6bb4-7d49-4a09-9121-2ef995211fe2\n",
      "                  ➜ Company names with location and industry\n",
      "                    ↪ID:  691f6af5-9afb-4872-a9eb-e44164a501f8\n",
      "                  ➜ HR data with PII\n",
      "                    ↪ID:  842d0169-f579-4a70-864c-0374fe39aeea\n",
      "                  ➜ 500 americans\n",
      "                    ↪ID:  8910f376-25c8-43ab-bc7f-ad14946780c5\n",
      "                  ➜ 500 australians\n",
      "                    ↪ID:  d1524dc3-fb76-46ce-a3c8-5037d2712acf\n",
      "                  ➜ Customer data with PII\n",
      "                    ↪ID:  ea665978-ab26-482d-bbf1-a288667bd410\n",
      "                  ➜ Unstructured data with PII\n",
      "                    ↪ID:  f03282b0-65b5-4017-aa61-349b3a8039d1\n",
      "                  ➜ More names and locations\n",
      "                    ↪ID:  ff9e968a-79ca-4f42-b24a-9e80b666699a\n",
      "\n",
      "\u001b[1mCollection Name:  CDO data\n",
      "\u001b[0m             ID:  7ab428b0-e401-46ab-9063-1608466f1bb7\n",
      "       Contains: \n",
      "                  ➜ Moody moody moodies\n",
      "                    ↪ID:  7604b344-a016-4b84-b98c-ed3f6da7f40d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking that the collection was created\n",
    "\n",
    "for dataset in dtl.datastore_collection.list():\n",
    "    print('\\033[1m' \"Collection Name: \", dataset.name) \n",
    "    print('\\033[0m' \"             ID: \", dataset.id)\n",
    "    print(\"       Contains: \", )\n",
    "    for datastore in dataset.storeIds:\n",
    "        print(\"                  ➜ \" + datastore[\"name\"])\n",
    "        print(\"                    ↪ID: \", datastore[\"id\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Ontology'></a>\n",
    "# Create ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wireless_ontology = Ontology(\n",
    "    \"Declassified Wireless Carrier Data\",\n",
    "    \"This is for the purpose of cleaning and delivering safe data as a product.\",\n",
    "    [\n",
    "        OntologyNode(\n",
    "            \"Sensitive Data\",\n",
    "            \"This is data NOT to be distributed to 3rd parties.\",\n",
    "            [\n",
    "                OntologyNode(\n",
    "                    \"Subscriber\",\n",
    "                    None,\n",
    "                    [\n",
    "                        OntologyNode(\"Full Name\"),\n",
    "                        OntologyNode(\"First Name\"),\n",
    "                        OntologyNode(\"Family Name\"),\n",
    "                        OntologyNode(\"Subscriber Company\"),\n",
    "                        OntologyNode(\"Account Identifier\"),\n",
    "                        OntologyNode(\n",
    "                            \"Address\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Subscriber Street\"),\n",
    "                                OntologyNode(\"Subscriber City\"),\n",
    "                                OntologyNode(\"Subscriber Zip\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\"Subscriber Phone Number\"),\n",
    "                        OntologyNode(\"Subscriber Email\"),\n",
    "                    ],\n",
    "                ),\n",
    "                OntologyNode(\n",
    "                    \"Sensitive Device Data\",\n",
    "                    None,\n",
    "                    [\n",
    "                        OntologyNode(\n",
    "                            \"Sensitive Device Identifier\",\n",
    "                            None,\n",
    "                            [OntologyNode(\"Mac Address\"), \n",
    "                             OntologyNode(\"IMEI\")],\n",
    "                        ),\n",
    "                        OntologyNode(\n",
    "                            \"Sensitive Device Telemetry\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\n",
    "                                    \"Sensitive Geospatial\",\n",
    "                                    None,\n",
    "                                    [\n",
    "                                        OntologyNode(\"Latitude\"),\n",
    "                                        OntologyNode(\"Longitude\"),\n",
    "                                        OntologyNode(\"Altitude\"),\n",
    "                                        OntologyNode(\"Geo Hash\"),\n",
    "                                    ],\n",
    "                                ),\n",
    "                                OntologyNode(\"IP Address\"),\n",
    "                                OntologyNode(\"Mobile Network Code\", \"MNC\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "                OntologyNode(\n",
    "                    \"Commercially Sensitive Information\",\n",
    "                    \"Information that providers or manufacturers may not want released\",\n",
    "                    [\n",
    "                        OntologyNode(\"Network Provider\"),\n",
    "                        OntologyNode(\"Manufacturer\"),\n",
    "                        OntologyNode(\"Device ID\"),\n",
    "                        OntologyNode(\"OS\"),\n",
    "                        OntologyNode(\"Device Model\"),\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        OntologyNode(\n",
    "            \"Data to Distribute\",\n",
    "            \"Wireless carrier data that is not sensitive and can be used for public consumption\",\n",
    "            [\n",
    "                OntologyNode(\n",
    "                    \"Device Data\",\n",
    "                    None,\n",
    "                    [\n",
    "                        OntologyNode(\n",
    "                            \"Screen Resolution\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Screen Resolution Width\"),\n",
    "                                OntologyNode(\"Screen Resolution Height\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\"Device Memory\"),\n",
    "                        OntologyNode(\"Device Storage\"),\n",
    "                        OntologyNode(\"Device Language\"),\n",
    "                    ],\n",
    "                ),\n",
    "                OntologyNode(\n",
    "                    \"Device Telemetry\",\n",
    "                    None,\n",
    "                    [\n",
    "                        OntologyNode(\n",
    "                            \"Device Hardware Performance\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Device System Uptime\"),\n",
    "                                OntologyNode(\"Device Used Storage\"),\n",
    "                                OntologyNode(\"Device Unused Storage\"),\n",
    "                                OntologyNode(\"Device Used Memory\"),\n",
    "                                OntologyNode(\"Device Unused Memory\"),\n",
    "                                OntologyNode(\"Device CPU Usage\"),\n",
    "                                OntologyNode(\"Device Battery Usage\"),\n",
    "                                OntologyNode(\"Device Battery State\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\n",
    "                            \"Device Geospatial Data\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\n",
    "                                    \"Device Horizontal Accuracy\",\n",
    "                                    \"Device Horizontal GPS Accuracy\",\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Device Vertical Accuracy\",\n",
    "                                    \"Device Vertical GPS Accuracy\",\n",
    "                                ),\n",
    "                                OntologyNode(\"Device Velocity Speed\"),\n",
    "                                OntologyNode(\"Device Velocity Bearing\"),\n",
    "                                OntologyNode(\"Session State or Territory\"),\n",
    "                                OntologyNode(\"Session Country\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "                OntologyNode(\n",
    "                    \"Network Telemetry\",\n",
    "                    None,\n",
    "                    [\n",
    "                        OntologyNode(\n",
    "                            \"Session Information\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Session Connection Type\"),\n",
    "                                OntologyNode(\"Session Connection Technology\"),\n",
    "                                OntologyNode(\n",
    "                                    \"Session Time\",\n",
    "                                    None,\n",
    "                                    [\n",
    "                                        OntologyNode(\"Session Start\"),\n",
    "                                        OntologyNode(\"Session End\"),\n",
    "                                        OntologyNode(\"Session Timezone\"),\n",
    "                                        OntologyNode(\"QoS Timestamp\"),\n",
    "                                    ],\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Session Delta Transmitted Bytes\",\n",
    "                                    \"Delta from last session\",\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Session Delta Received Bytes\",\n",
    "                                    \"Delta from last session\",\n",
    "                                ),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\n",
    "                            \"Mobile Network\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Mobile Connection Generation\"),\n",
    "                                OntologyNode(\"Mobile Channel\"),\n",
    "                                OntologyNode(\"Mobile Country Code\", \"MCC\"),\n",
    "                                OntologyNode(\"Base Station Identity Code\", \"BSIC\"),\n",
    "                                OntologyNode(\n",
    "                                    \"Physical Cell Identifier\", \"PCI, LTE Only\"\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Reference Signal Received Power\", \"RSRP, LTE Only\"\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Reference Signal Received Quality\",\n",
    "                                    \"RSRQ, LTE Only\",\n",
    "                                ),\n",
    "                                OntologyNode(\n",
    "                                    \"Reference Signal Signal to Noise Ratio\",\n",
    "                                    \"RSSNR, LTE Only\",\n",
    "                                ),\n",
    "                                OntologyNode(\"Channel Quality Indicator\", \"CQI\"),\n",
    "                                OntologyNode(\"Timing Advance\", \"TA - high is far\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\n",
    "                            \"WiFi Network\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"SSID\"),\n",
    "                                OntologyNode(\"WiFi Channel\"),\n",
    "                                OntologyNode(\"WiFi Encryption\"),\n",
    "                                OntologyNode(\"Modulation and Coding Scheme\", \"MCS\"),\n",
    "                                OntologyNode(\"BSSID\"),\n",
    "                                OntologyNode(\"Wifi Frequency\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        OntologyNode(\n",
    "                            \"Quality of Service\",\n",
    "                            None,\n",
    "                            [\n",
    "                                OntologyNode(\"Upload Throughput\"),\n",
    "                                OntologyNode(\"Download Throughput\"),\n",
    "                                OntologyNode(\"Latency Average\"),\n",
    "                                OntologyNode(\"Link Speed\"),\n",
    "                                OntologyNode(\"Signal Strength\"),\n",
    "                                OntologyNode(\"Jitter\", \"Variance of latency\"),\n",
    "                                OntologyNode(\"Packet Loss\", \"Lost Percentage\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wireless_ontology = dtl.ontology.create(wireless_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training_Data'></a>\n",
    "## Attach training data\n",
    "\n",
    "Attaching training data from telemetry source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get ontology leaves\n",
    "\n",
    "leaves = wireless_ontology.leaves()\n",
    "\n",
    "# for idx, leaf in enumerate(leaves):\n",
    "#    print(idx, \"|\", leaf.node_id, \"|\", leaf.name, \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create leaves dictionary\n",
    "\n",
    "leaf_node_dict = {}\n",
    "for leaf in leaves:\n",
    "    leaf_node_dict[leaf.name] = leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying data stores for reference\n",
    "\n",
    "datastore_list = dtl.datastore.list() # this would be better as a dictionary so we can use it programatically\n",
    "    \n",
    "# for i in range(len(datastore_list)):\n",
    "#     print(\"➜ \", datastore_list[i].id, \"|\", datastore_list[i].name)\n",
    "# print('\\n Currently total of ', len(datastore_list), ' datastores available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDatastoreID (datastoreName):\n",
    "    \"\"\"\n",
    "    Retrieves store ID by reference to store name. If there are multiple identically named stores, returns ID of first one. Note: it is bad practice to have multiple identical data stores\n",
    "    \n",
    "    :param datastoreName: Name of datastore to be added\n",
    "    \"\"\"\n",
    "    return list(filter(lambda d: d.name == datastoreName, datastore_list))[0].id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLeafNodeDict(ontology):\n",
    "    \"\"\"\n",
    "    Takes ontology and creates a leaf node dictionary for use in attaching training data\n",
    "    \n",
    "    :param ontology: Ontology to be used below\n",
    "    \"\"\"\n",
    "    \n",
    "    # get ontology leaves\n",
    "    \n",
    "    leaves = wireless_ontology.leaves()\n",
    "    \n",
    "    # create leaves dictionary\n",
    "    \n",
    "    leaf_node_dict = {}\n",
    "    for leaf in leaves:\n",
    "        leaf_node_dict[leaf.name] = leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateLeafNodeDict(wireless_ontology) # Note, probably best to shove into AttachTrainingDataDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttachTrainingDataDictionary(col_dictionary, store_path, store_name, ontology_dict = leaf_node_dict):\n",
    "    \"\"\"\n",
    "    This takes a mapping of the columns in a data store to an ontology, the details of a data store and the details of an ontology and attaches the data to the ontology as training data.\n",
    "\n",
    "    Requires function GetStoreId()\n",
    "    Requires col_dictionary\n",
    "    Requires leaf_node_dict created with function CreateLeafNodeDict()\n",
    "    \n",
    "    :param col_dictionary: a dictionary mapping the ontology to the data store, structured {\"Ontology Leaf Node Name\": \"column_Name\"}\n",
    "    :param store_path: the path to the datastore\n",
    "    :param store_name: name of data store\n",
    "    :param ontology_dict: dictionary which has the leaf nodes of the relevant ontology, defined with CreateLeafNodeDict(ontology)\n",
    "    \"\"\"\n",
    "    for leaf in col_dictionary:\n",
    "        col = col_dictionary[leaf]\n",
    "        path = ([store_path, col])\n",
    "        node = ontology_dict[leaf]\n",
    "        ref = DataRef(node, [path])\n",
    "        stream_id_for_data_transfer = dtl.training.data.add(\n",
    "            GetDatastoreID(store_name), store_name, [ref]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details about data store\n",
    "\n",
    "us_telemetry_data_store_name = \"US Telemetry 5k\"\n",
    "us_telemetry_data_store_id = GetDatastoreID(us_telemetry_data_store_name)\n",
    "us_telemetry_data_path = \"5K_us_telemetry.csv\"\n",
    "\n",
    "# ontology nodes, and training data columns\n",
    "\n",
    "# us_telemetry_training_col_dict = {\n",
    "#     \"Packet Loss\": \"main_QOS_PacketLoss_LostPercentage\",\n",
    "#     \"Jitter\": \"main_QOS_Jitter_Average\",\n",
    "#     \"Signal Strength\": \"main_QOS_SignalStrength\",\n",
    "#     \"Link Speed\": \"main_QOS_LinkSpeed\",\n",
    "#     \"Latency Average\": \"main_QOS_Latency_Average\",\n",
    "#     \"Download Throughput\": \"main_QOS_DownloadThroughput\",\n",
    "#     \"Upload Throughput\": \"main_QOS_UploadThroughput\",\n",
    "#     \"Wifi Frequency\": \"main_WifiFrequency\",\n",
    "#     \"BSSID\": \"main_BSSID\",\n",
    "#     \"Timing Advance\": \"main_QOS_TA\",\n",
    "#     \"Channel Quality Indicator\": \"main_QOS_CQI\",\n",
    "#     \"Reference Signal Signal to Noise Ratio\": \"main_QOS_RSSNR\",\n",
    "#     \"Reference Signal Received Quality\": \"main_QOS_RSRQ\",\n",
    "#     \"Reference Signal Received Power\": \"main_QOS_RSRP\",\n",
    "#     \"Physical Cell Identifier\": \"main_PCI\",\n",
    "#     \"Base Station Identity Code\": \"main_BSIC\",\n",
    "#     \"Mobile Country Code\": \"main_MCC\",\n",
    "#     \"Mobile Channel\": \"main_MobileChannel\",\n",
    "#     \"Mobile Connection Generation\": \"conn_Generation_Category\",\n",
    "#     \"Session Delta Received Bytes\": \"main_QOS_DeltaReceivedBytes\",\n",
    "#     \"Session Delta Transmitted Bytes\": \"main_QOS_DeltaTransmittedBytes\",\n",
    "#     \"QoS Timestamp\": \"main_QOS_QOSDate\",\n",
    "#     \"Session Timezone\": \"main_Timezone\",\n",
    "#     \"Session End\": \"main_ConnectionEnd\",\n",
    "#     \"Session Start\": \"main_ConnectionStart\",\n",
    "#     \"Session Connection Technology\": \"main_ConnectionTechnology\",\n",
    "#     \"Session Connection Type\": \"main_ConnectionType\",\n",
    "#     \"Session Country\": \"main_Country\",\n",
    "#     \"Device Velocity Bearing\": \"main_QOS_Velocity_Bearing\",\n",
    "#     \"Device Velocity Speed\": \"main_QOS_Velocity_Speed\",\n",
    "#     \"Device Vertical Accuracy\": \"main_QOS_Location_VerticalAccuracy\",\n",
    "#     \"Device Horizontal Accuracy\": \"main_QOS_Location_HorizontalAccuracy\",\n",
    "#     \"Device Battery State\": \"main_QOS_DeviceBatteryState\",\n",
    "#     \"Device Battery Usage\": \"main_QOS_DeviceBatteryLevel\",\n",
    "#     \"Device CPU Usage\": \"main_QOS_DeviceCPU\",\n",
    "#     \"Device Unused Memory\": \"main_QOS_DeviceFreeMemory\",\n",
    "#     \"Device Used Memory\": \"main_QOS_DeviceUsedMemory\",\n",
    "#     \"Device Unused Storage\": \"main_QOS_DeviceFreeStorage\",\n",
    "#     \"Device Used Storage\": \"main_QOS_DeviceUsedStorage\",\n",
    "#     \"Device System Uptime\": \"main_QOS_SystemUptime\",\n",
    "#     \"Device Language\": \"main_Device_DeviceLanguage\",\n",
    "#     \"Device Storage\": \"main_Device_Storage\",\n",
    "#     \"Device Memory\": \"main_Device_Memory\",\n",
    "#     \"Screen Resolution Height\": \"main_Device_ScreenResolution_Height\",\n",
    "#     \"Screen Resolution Width\": \"main_Device_ScreenResolution_Width\",\n",
    "#     \"Device Model\": \"main_Device_Model\",\n",
    "#     \"OS\": \"main_Device_OS\",\n",
    "#     \"Manufacturer\": \"main_Device_Manufacturer\",\n",
    "#     \"Network Provider\": \"sp_ServiceProviderBrandName\",\n",
    "#     \"Network Provider\": \"main_ServiceProvider\",\n",
    "#     \"Mobile Network Code\": \"main_MNC\",\n",
    "#     \"Geo Hash\": \"main_Geohash\",\n",
    "#     \"Altitude\": \"main_QOS_Location_Altitude\",\n",
    "#     \"Longitude\": \"main_QOS_Location_Longitude\",\n",
    "#     \"Latitude\": \"main_QOS_Location_Latitude\",\n",
    "#     \"Subscriber City\": \"main_City\",\n",
    "#  }\n",
    "\n",
    "# attaches above as training data\n",
    "\n",
    "# for leaf in us_telemetry_training_col_dict:\n",
    "#     col = us_telemetry_training_col_dict[leaf]\n",
    "#     path = ([us_telemetry_data_path, col])\n",
    "#     node = leaf_node_dict[leaf]\n",
    "#     ref = DataRef(node, [path])\n",
    "#     us_telemetry_stream_id_for_data_transfer = dtl.training.data.add(GetDatastoreID(us_telemetry_data_store_name), us_telemetry_data_store_name, [ref])\n",
    "\n",
    "# AttachTrainingDataDictionary(\n",
    "#     us_telemetry_training_col_dict,\n",
    "#     us_telemetry_data_path,\n",
    "#     us_telemetry_data_store_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict2 = {\n",
    "    \"Packet Loss\": \"main_QOS_PacketLoss_LostPercentage\",\n",
    "    \"Jitter\": \"main_QOS_Jitter_Average\",\n",
    "    \"Signal Strength\": \"main_QOS_SignalStrength\",\n",
    "    \"Link Speed\": \"main_QOS_LinkSpeed\",\n",
    "    \"Latency Average\": \"main_QOS_Latency_Average\",\n",
    "    \"Download Throughput\": \"main_QOS_DownloadThroughput\",\n",
    "    \"Upload Throughput\": \"main_QOS_UploadThroughput\",\n",
    "    \"Wifi Frequency\": \"main_WifiFrequency\",\n",
    "    \"BSSID\": \"main_BSSID\",\n",
    "    \"Timing Advance\": \"main_QOS_TA\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict2,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict3 = {\n",
    "    \"Channel Quality Indicator\": \"main_QOS_CQI\",\n",
    "    \"Reference Signal Signal to Noise Ratio\": \"main_QOS_RSSNR\",\n",
    "    \"Reference Signal Received Quality\": \"main_QOS_RSRQ\",\n",
    "    \"Reference Signal Received Power\": \"main_QOS_RSRP\",\n",
    "    \"Physical Cell Identifier\": \"main_PCI\",\n",
    "    \"Base Station Identity Code\": \"main_BSIC\",\n",
    "    \"Mobile Country Code\": \"main_MCC\",\n",
    "    \"Mobile Channel\": \"main_MobileChannel\",\n",
    "    \"Mobile Connection Generation\": \"conn_Generation_Category\",\n",
    "    \"Session Delta Received Bytes\": \"main_QOS_DeltaReceivedBytes\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict3,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict4 = {\n",
    "    \"Session Delta Transmitted Bytes\": \"main_QOS_DeltaTransmittedBytes\",\n",
    "    \"QoS Timestamp\": \"main_QOS_QOSDate\",\n",
    "    \"Session Timezone\": \"main_Timezone\",\n",
    "    \"Session End\": \"main_ConnectionEnd\",\n",
    "    \"Session Start\": \"main_ConnectionStart\",\n",
    "    \"Session Connection Technology\": \"main_ConnectionTechnology\",\n",
    "    \"Session Connection Type\": \"main_ConnectionType\",\n",
    "    \"Session Country\": \"main_Country\",\n",
    "    \"Device Velocity Bearing\": \"main_QOS_Velocity_Bearing\",\n",
    "    \"Device Velocity Speed\": \"main_QOS_Velocity_Speed\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict4,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict5 = {\n",
    "    \"Device Vertical Accuracy\": \"main_QOS_Location_VerticalAccuracy\",\n",
    "    \"Device Horizontal Accuracy\": \"main_QOS_Location_HorizontalAccuracy\",\n",
    "    \"Device Battery State\": \"main_QOS_DeviceBatteryState\",\n",
    "    \"Device Battery Usage\": \"main_QOS_DeviceBatteryLevel\",\n",
    "    \"Device CPU Usage\": \"main_QOS_DeviceCPU\",\n",
    "    \"Device Unused Memory\": \"main_QOS_DeviceFreeMemory\",\n",
    "    \"Device Used Memory\": \"main_QOS_DeviceUsedMemory\",\n",
    "    \"Device Unused Storage\": \"main_QOS_DeviceFreeStorage\",\n",
    "    \"Device Used Storage\": \"main_QOS_DeviceUsedStorage\",\n",
    "    \"Device System Uptime\": \"main_QOS_SystemUptime\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict5,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict6 = {\n",
    "    \"Device Language\": \"main_Device_DeviceLanguage\",\n",
    "    \"Device Storage\": \"main_Device_Storage\",\n",
    "    \"Device Memory\": \"main_Device_Memory\",\n",
    "    \"Screen Resolution Height\": \"main_Device_ScreenResolution_Height\",\n",
    "    \"Screen Resolution Width\": \"main_Device_ScreenResolution_Width\",\n",
    "    \"Device Model\": \"main_Device_Model\",\n",
    "    \"OS\": \"main_Device_OS\",\n",
    "    \"Manufacturer\": \"main_Device_Manufacturer\",\n",
    "    \"Network Provider\": \"sp_ServiceProviderBrandName\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict6,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_telemetry_training_col_dict7 = {\n",
    "    \"Network Provider\": \"main_ServiceProvider\",\n",
    "    \"Mobile Network Code\": \"main_MNC\",\n",
    "    \"Geo Hash\": \"main_Geohash\",\n",
    "    \"Altitude\": \"main_QOS_Location_Altitude\",\n",
    "    \"Longitude\": \"main_QOS_Location_Longitude\",\n",
    "    \"Latitude\": \"main_QOS_Location_Latitude\",\n",
    "    \"Subscriber City\": \"main_City\",\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_telemetry_training_col_dict7,\n",
    "    us_telemetry_data_path,\n",
    "    us_telemetry_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching training data with other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canadian Names datastore\n",
    "\n",
    "canadian_names_data_store_name = \"Canadian Names\"\n",
    "canadian_names_data_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500canadians.csv\"\n",
    "\n",
    "canadian_names_col_dict = {\n",
    "'First Name': 'first_name',\n",
    "'Family Name': 'last_name',\n",
    "'Subscriber Street': 'address',\n",
    "'Subscriber Company': 'company_name',\n",
    "'Subscriber City': 'city',\n",
    "'Session State or Territory': 'province',\n",
    "'Subscriber Zip': 'postal',\n",
    "'Subscriber Phone Number': 'phone1',\n",
    "'Subscriber Phone Number': 'phone2',\n",
    "'Subscriber Email': 'email',\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    canadian_names_col_dict,\n",
    "    canadian_names_data_path,\n",
    "    canadian_names_data_store_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# British Names datastore\n",
    "\n",
    "british_names_data_store_name = \"British Names\"\n",
    "british_names_data_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500brits.csv\"\n",
    "\n",
    "british_names_col_dict = {\n",
    "'First Name': 'first_name',\n",
    "'Family Name': 'last_name',\n",
    "'Subscriber Company': 'company_name',\n",
    "'Subscriber Street': 'address',\n",
    "'Subscriber City': 'city',\n",
    "'Session State or Territory': 'county',\n",
    "'Subscriber Zip': 'postal',\n",
    "'Subscriber Phone Number': 'phone1',\n",
    "'Subscriber Phone Number': 'phone2',\n",
    "'Subscriber Email': 'email',\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    british_names_col_dict,\n",
    "    british_names_data_path,\n",
    "    british_names_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Names datastore\n",
    "\n",
    "us_names_data_store_name = \"US Names\"\n",
    "us_names_data_store_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/500americans.csv\"\n",
    "\n",
    "us_names_col_dict = {\n",
    "'First Name': 'first_name',\n",
    "'Family Name': 'last_name',\n",
    "'Subscriber Company': 'company_name',\n",
    "'Subscriber Street': 'address',\n",
    "'Subscriber City': 'city',\n",
    "'Session State or Territory': 'county',\n",
    "'Session State or Territory': 'state',\n",
    "'Subscriber Zip': 'zip',\n",
    "'Subscriber Phone Number': 'phone1',\n",
    "'Subscriber Phone Number': 'phone2',\n",
    "'Subscriber Email': 'email',\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    us_names_col_dict,\n",
    "    us_names_data_store_path,\n",
    "    us_names_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DtlError' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-79f6cab132fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0musers_col_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0musers_data_store_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0musers_data_store_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-19-4e441f8a9ac7>\u001b[0m in \u001b[0;36mAttachTrainingDataDictionary\u001b[0;34m(col_dictionary, store_path, store_name, ontology_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         stream_id_for_data_transfer = dtl.training.data.add(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mGetDatastoreID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/datalogue/clients/training.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, store_id, store_name, refs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mstream_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transfer_data_from_datastore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataRef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDtlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mstream_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/datalogue/clients/training.py\u001b[0m in \u001b[0;36m__transfer_data_from_datastore\u001b[0;34m(self, store_id, dataset_id, node_id, path)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_authed_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHttpMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mstream_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUUID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"streamId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DtlError' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Users datastore\n",
    "\n",
    "users_data_store_name = \"Users\"\n",
    "users_data_store_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/pii/users.csv\"\n",
    "\n",
    "users_col_dict = {\n",
    "'First Name': \"first_name\",\n",
    "'Family Name': \"last_name\",\n",
    "'Subscriber Company': 'company_name',\n",
    "'Subscriber Street': 'address',\n",
    "'Subscriber City': 'city',\n",
    "'Session State or Territory': 'county',\n",
    "'Session State or Territory': 'state',\n",
    "'Subscriber Zip': 'zip',\n",
    "'Subscriber Phone Number': 'phone1',\n",
    "'Subscriber Phone Number': 'phone',\n",
    "'Subscriber Email': 'email',\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    users_col_dict,\n",
    "    users_data_store_path,\n",
    "    users_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries datastore\n",
    "\n",
    "countries_data_store_name = \"Countries\"\n",
    "countries_data_store_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/countries.csv\"\n",
    "\n",
    "countries_col_dict = {\n",
    "'Session Country': 'Name',\n",
    "'Session Country': 'Code'\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    countries_col_dict,\n",
    "    countries_data_store_path,\n",
    "    countries_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Cities datastore\n",
    "\n",
    "some_cities_data_store_name = \"Some Cities\"\n",
    "some_cities_data_store_path = \"https://raw.githubusercontent.com/datalogue/demo-data/master/people_places/cities.csv\"\n",
    "\n",
    "some_cities_col_dict = {\n",
    "'Subscriber City': 'City',\n",
    "'Session State or Territory': 'State'\n",
    "}\n",
    "\n",
    "AttachTrainingDataDictionary(\n",
    "    some_cities_col_dict,\n",
    "    some_cities_data_store_path,\n",
    "    some_cities_data_store_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training'></a>\n",
    "# Train a model\n",
    "\n",
    "Warning, this is GPU intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Datalogue Semantic Engine to train a classification model on the Telemetry Ontology \n",
    "\n",
    "# dtl.training.run(\n",
    "#   ontology_id = wireless_ontology.ontology_id # will change to wireless_ontology.id in next version\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training data view](images/metrics.png)\n",
    "\n",
    "Model metrics view showing above training in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Manual step currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve training\n",
    "\n",
    "list_of_trainings = dtl.training.get_trainings(\n",
    "  ontology_id = wireless_ontology.ontology_id, # will change to wireless_ontology.id in next version\n",
    ")\n",
    "\n",
    "for training in list_of_trainings:\n",
    "    print(\"Training for ontology id: \" + str(training.ontology_id))\n",
    "    print(\"   \" + training.status)\n",
    "    print(\"   \" + training.start_time)\n",
    "    print(\"   \" + training.model_type)\n",
    "\n",
    "\n",
    "\n",
    "# deploy model\n",
    "## Note: this is in version 0.9 of the General Release, for now, use GUI\n",
    "\n",
    "# dtl.training.deploy(, wireless_ontology.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Metrics](images/metrics2.png)\n",
    "\n",
    "Model metrics view showing performance of above model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Pipelines'></a>\n",
    "# Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach large datastores to be inferred upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a datastore: European Sample Dataset\n",
    "\n",
    "gcsdef_2 = GCSDatastoreDef(\n",
    "    bucket = \"dtl-handset-telemetry\",\n",
    "    file_name = \"5K_eu_telemetry.csv\",\n",
    "    file_format = FileFormat.Csv,\n",
    ")\n",
    "\n",
    "EU_sample_handset_data = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name = \"European Handset Telemetry Sample\", \n",
    "        definition = gcsdef_2, \n",
    "        credential_id = my_gcs_credentials.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a datastore: European Fullscale Dataset\n",
    "\n",
    "gcsdef_3 = GCSDatastoreDef(\n",
    "    bucket = \"dtl-handset-telemetry\",\n",
    "    file_name = \"gtc_europe.csv\",\n",
    "    file_format = FileFormat.Csv,\n",
    ")\n",
    "\n",
    "EU_fullscale_handset_data = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name = \"European Handset Telemetry\", \n",
    "        definition = gcsdef_3, \n",
    "        credential_id = my_gcs_credentials.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a datastore: US Fullscale Dataset\n",
    "\n",
    "gcsdef_4 = GCSDatastoreDef(\n",
    "    bucket = \"dtl-handset-telemetry\",\n",
    "    file_name = \"us_340M.csv\",\n",
    "    file_format = FileFormat.Csv,\n",
    ")\n",
    "\n",
    "US_fullscale_handset_data = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name = \"US Handset Telemetry\", \n",
    "        definition = gcsdef_4, \n",
    "        credential_id = my_gcs_credentials.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect into datastore collection\n",
    "\n",
    "dtl.datastore_collection.create(\n",
    "    DatastoreCollection(\n",
    "        name = \"Handset telemetry data for inference\",\n",
    "        description = \"Handset training data for use in model creation\",\n",
    "        storeIds = [EU_sample_handset_data.id, EU_fullscale_handset_data.id, US_fullscale_handset_data.id],\n",
    "    )\n",
    ")\n",
    "\n",
    "BOT.server_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the collection was created\n",
    "\n",
    "for dataset in dtl.datastore_collection.list():\n",
    "    print('\\033[1m' \"Collection Name: \", dataset.name) \n",
    "    print('\\033[0m' \"             ID: \", dataset.id)\n",
    "    print(\"       Contains: \", )\n",
    "    for datastore in dataset.storeIds:\n",
    "        print(\"                  ➜ \" + datastore[\"name\"])\n",
    "        print(\"                    ↪ID: \", datastore[\"id\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Datastore Colletion view](images/Datastore_collection3.png)\n",
    "\n",
    "Preview of the Datastore collection above in the Datalogue interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze datastore in GUI before pipelining (see comment below)\n",
    "\n",
    "Analyse datastore\n",
    "This allows us to classify only the first chunk of a dataset, to spare GPU resources if a schema is static. do this in the GUI by going into the data store details through the settings menu (three dots on the top right) then select the dataset, and hit \"analyse\" in the three dots menu from that datastore.\n",
    "\n",
    "![Analyze view](images/analyze.png)\n",
    "\n",
    "Analysis showing inferred classes on European and US datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source\n",
    "source1 = EU_sample_handset_data\n",
    "\n",
    "# Define destination\n",
    "\n",
    "gcsdef_5 = GCSDatastoreDef(\n",
    "    bucket=\"dtl-handset-telemetry\",\n",
    "    file_name=\"cleaned_handset_telemetry_data.csv\",\n",
    "    file_format=FileFormat.Csv,\n",
    ")\n",
    "\n",
    "target1 = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name=\"Cleaned Handset Telemetry data\",\n",
    "        definition=gcsdef_5,\n",
    "        credential_id=my_gcs_credentials.id,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of class nodes for use in structure transformation\n",
    "class_nodes_list = []\n",
    "\n",
    "# populate list with leaves\n",
    "\n",
    "for leaf in leaves:\n",
    "    class_nodes_list.append(\n",
    "        ClassNodeDescription(\n",
    "            path=[leaf.name],\n",
    "            tag=leaf.name,\n",
    "            pick_strategy=PickStrategy.HighScore,\n",
    "            data_type=DataType.String,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "definition1 = Definition(\n",
    "    transformations=[\n",
    "#        Classify(None, True), # If you have the analyze above, you don't need to classify. It is much more efficient to do this.\n",
    "        Structure(\n",
    "            class_nodes_list\n",
    "        ),\n",
    "    ],\n",
    "    pipelines=[],\n",
    "    target=target1,\n",
    ")\n",
    "\n",
    "# Define stream\n",
    "my_stream = Stream(source1, [definition1])\n",
    "\n",
    "# Push\n",
    "\n",
    "stream_collection = dtl.stream_collection.create(\n",
    "    [my_stream], \"Telemetry classification pipeline\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline view](images/pipeline.png)\n",
    "\n",
    "Pipeline view showing classification and structure transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source\n",
    "source2 = EU_fullscale_handset_data\n",
    "\n",
    "# Define destination\n",
    "\n",
    "gcsdef_5 = GCSDatastoreDef(\n",
    "    bucket=\"dtl-handset-telemetry\",\n",
    "    file_name=\"cleaned_EU_handset_telemetry_data.csv\",\n",
    "    file_format=FileFormat.Csv,\n",
    ")\n",
    "\n",
    "target1 = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name=\"Cleaned EU Handset Telemetry data\",\n",
    "        definition=gcsdef_5,\n",
    "        credential_id=my_gcs_credentials.id,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "definition1 = Definition(\n",
    "    transformations=[\n",
    "#        Classify(None, True), # If you have the analyze above, you don't need to classify. It is much more efficient to do this.\n",
    "        Structure(\n",
    "            class_nodes_list\n",
    "        ),\n",
    "    ],\n",
    "    pipelines=[],\n",
    "    target=target1,\n",
    ")\n",
    "\n",
    "# Define stream\n",
    "my_stream = Stream(source2, [definition1])\n",
    "\n",
    "# Push\n",
    "\n",
    "stream_collection = dtl.stream_collection.create(\n",
    "    [my_stream], \"EU Telemetry classification pipeline\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source\n",
    "source3 = US_fullscale_handset_data\n",
    "\n",
    "# Define destination\n",
    "\n",
    "gcsdef_5 = GCSDatastoreDef(\n",
    "    bucket=\"dtl-handset-telemetry\",\n",
    "    file_name=\"cleaned_US_handset_telemetry_data.csv\",\n",
    "    file_format=FileFormat.Csv,\n",
    ")\n",
    "\n",
    "target1 = dtl.datastore.create(\n",
    "    Datastore(\n",
    "        name=\"Cleaned US Handset Telemetry data\",\n",
    "        definition=gcsdef_5,\n",
    "        credential_id=my_gcs_credentials.id,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "definition1 = Definition(\n",
    "    transformations=[\n",
    "#        Classify(None, True), # If you have the analyze above, you don't need to classify. It is much more efficient to do this.\n",
    "        Structure(\n",
    "            class_nodes_list\n",
    "        ),\n",
    "    ],\n",
    "    pipelines=[],\n",
    "    target=target1,\n",
    ")\n",
    "\n",
    "# Define stream\n",
    "my_stream = Stream(source3, [definition1])\n",
    "\n",
    "# Push\n",
    "\n",
    "stream_collection = dtl.stream_collection.create(\n",
    "    [my_stream], \"US Telemetry classification pipeline\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_collection._as_payload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl.jobs.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtl.jobs.list()[3].percentage_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
